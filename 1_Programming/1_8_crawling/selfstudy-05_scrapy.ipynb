{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "- https://scrapy.org/\n",
    "- scrapy 기본 구조\n",
    "- xpath\n",
    "- Scrapy Project\n",
    "- Scrapy Excute\n",
    "- Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrapabsy의 장점 : request, bs4, selenium등보다 좀 더 빠르다.\n",
    "### 크롤링을 할 수 있는 데이터의 종류 관련해서는 차별점이 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapy 기본 구조\n",
    "\n",
    "- Spider\n",
    "    - 어떤 웹사이트들을 어떻게 크롤링할 것인지 명시하고, 각각의 웹 페이지의 어떤 부분을 스크래핑할 것인지 명시하는 클래스\n",
    "\n",
    "- Selector\n",
    "    - 웹 페이지상의 특정 HTML Element를 간편하게 선택할 수 있도록 하는 메커니즘을 구현한 클래스\n",
    "    - CSS Selector를 직접 사용하거나, XPath를 사용할 수 있음\n",
    "    - XPath를 사용하는것이 더 권장됨\n",
    "\n",
    "- items.py\n",
    "    - 웹페이지에서 원하는 부분을 스크랩하여 저장할 때 사용하는 사용자 정의 자료구조 클래스 (스크랩하고자 하는 항목들을 미리 정의하는 것 ex> discount rate는 뭐다~)\n",
    "\n",
    "- pipeline.py\n",
    "    - 스크래핑 결과물을 Item 형태로 구성하였을 때, 이를 자유롭게 가공하거나 다양한 파일 형태로 저장할 수 있도록 하는 클래스 (getter와 setter의 개념)\n",
    "\n",
    "- settings.py\n",
    "    - Spider나 Item Pipeline 등이 어떻게 동작하도록 할 지에 대한 세부적인 설정 사항을 기재하는 파일 크롤링의 빈도등을 설정\n",
    "    - cf. robots.txt (settings.py - ROBOTSTXT_OBEY = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
