{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 엔트로피의 정의\n",
    "- 확률분포들이 가지는 \"확신의 정도\"를 \"수치\"로 표현하는 것. <br>\n",
    "(\"분산되어 있는 정도\" : 여러가지로 확률이 골고루 분산 = Entrophy 증가 / 특정값으로 확률이 몰빵 = Entrophy 감소)\n",
    "- 한쪽으로 확 쏠릴 때 \"0\", 모든 확률 변수의 확률이 동일 : 엔트로피 최대 <br>\n",
    "    \n",
    "- 확률 변수 Y가 이산 확률 변수 일때 (베르누이, 카테고리 분포) <br>\n",
    "$ H[Y] = -\\sum_{k=1}^K p(y_k) \\log_2 p(y_k)$ : \"Y 이다\" 에 대한 확신의 정도 (기대값) <br>\n",
    "(K : 클래스의 개수, p(y) : 확률질량함수)\n",
    "- 확률 변수 Y 가 연속 확률 변수 일때 <br>\n",
    "$ H[Y] = -\\int p(y) \\log_2 p(y) \\; dy $\n",
    "(p(y) : 확률밀도함수)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7219280948873623\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#예제\n",
    "print(-0.5 * np.log2(0.5) - 0.5 * np.log2(0.5))\n",
    "print(-0.8 * np.log2(0.8) - 0.2 *np.log2(0.2))\n",
    "\n",
    "eps = np.finfo(float).eps  #log2(0)계산 시에는 eps를 선언해서 넣어준다.\n",
    "print(-1 * np.log2(1) - 0 * np.log2(eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 엔트로피의 성질\n",
    "- 1) 특정 하나 나올 확률 1, 나머지는 0 : $p(y_k)$ = 0, 엔트로피 0 (최소) <br>\n",
    "\n",
    "- 2) 이산확률 클래스 $2^k$개, 각 클래스 모두 같은 확률 : $H = -\\frac{2^K}{2^K}\\log_2\\dfrac{1}{2^K} = K$, 엔트로피 k (최대)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 엔트로피와 정보량\n",
    "- 엔트로피는 확률변수가 담을 수 있는 정보량을 표시.\n",
    "- 엔트로피 = 0 : 확률변수가 여러 값이 나올 수 없음 (추가정보 없음)\n",
    "- 엔트로피 = 1 : 확률변수가 여러 값이 나올 수 있음 (관측시 얻을 수 있는 추가정보 많음.) <br>\n",
    "<br>\n",
    "\n",
    "c.f) 가변길이 인코딩 : 나올 확률이 높은 변수와 낮은 변수의 인코딩 길이를 달리 함.\n",
    "    - 이때 엔트로피 : 각 확률변수들이 나올 때의 \"기대값\"과 동일. (가변길이 인코딩 할 때의 \"평균 통신량\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제 1 \n",
    "8글자 A,B,C,D,E,F,G,H에 대하여 각각의 글자가 나올 확률<br>\n",
    "$ \\Big\\{ \\dfrac{1}{2}, \\dfrac{1}{4}, \\dfrac{1}{8}, \\dfrac{1}{16}, \\dfrac{1}{64}, \\dfrac{1}{64}, \\dfrac{1}{64}, \\dfrac{1}{64} \\Big\\} $ <br>\n",
    "가변인코딩 방식 서술 및 한 글자 인코딩시의 평균 비트수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1/2 * np.log2(1/2) - 1/4 * np.log2(1/4) - 1/8 * np.log2(1/8) - 1/16 * np.log2(1/16) - ((1/64) * np.log2(1/64)) * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 표본 데이터가 주어진 경우 (실제 데이터)\n",
    "- 실제 데이터의 분포를 통해 p(y) (확률질량함수or확률밀도함수)를 추정, 엔트로피를 계산 <br>\n",
    "ex> 총 n = 80, y=0 : 40개, y=1 : 40개 <br>\n",
    "$ P(y=0) = \\dfrac{40}{80} = \\dfrac{1}{2} $\n",
    "$ P(y=1) = \\dfrac{40}{80} = \\dfrac{1}{2} $\n",
    "$ H[Y] = -\\dfrac{1}{2}\\log_2\\left(\\dfrac{1}{2}\\right) -\\dfrac{1}{2}\\log_2\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{2} + \\dfrac{1}{2}  = 1 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 조건부 엔트로피\n",
    "두 확률변수 X, Y에 대해, X의 값을 알 때, Y의 확률변수가 가질 수 있는 정보의 양\n",
    "\n",
    "\n",
    "* 공식<br>\n",
    "$ H[Y \\mid X] = - \\sum_i \\sum_j \\,p(x_i, y_j) \\log_2 p(y_j \\mid x_i) $  -> $H[Y \\mid X]  = \\sum_i \\,p(x_i)\\,H[Y \\mid x_i]$<br>\n",
    "$ H[Y \\mid X] = -\\int \\int p(x, y) \\log_2 p(y \\mid x) \\; dxdy $ -> $ H[Y \\mid X]  = \\int p(x)\\,H[Y \\mid x] \\; dx $<br>\n",
    "<br>\n",
    "\n",
    "* 결론 : 조건부 엔트로피는 X의 값에 의해 만들어지는 새로운 Y 확률분포의 가중평균이다. (조건부 확률에 log취해서 가중평균 (결합확률)\n",
    "* 의사결정나무 에서 조건부 엔트로피에서 진행하는 작업(Feature를 하나만 골라서 쪼개는)을 여러번 반복, Filtering을 여러번 해서 성능을 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 크로스 엔트로피 : 값이 작을수록 정답과 가깝고, 값이 클수록 오답이다.\n",
    "- 확률분포 $p(y), q(y) $의 크로스 엔트로피 구하기\n",
    "- 공식:\n",
    "    -$\\sum_{k=1}^K p(y_k) \\log_2 q(y_k)$ , $ -\\int p(y) \\log_2 q(y) dy $ <br>\n",
    "    (p(y) : Y의 분포 (정답),  p(q) : $\\hat{Y}$의 분포 (예측값))\n",
    "    -> 두 분포의 차이가 얼마나 나는가 에 대한 내용\n",
    "    \n",
    "- c.f> 이진 클래스 분류 문제 (Y = 0,1, $\\hat{Y}$ = $\\theta$ = P(Y=1) )에서 <br>\n",
    "C.E = $- y \\log_2 \\theta - (1 - y) \\log_2 (1 - \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 쿨백 - 라이블러 발산 (Kullback Leibler divergence) : 두 확률분포가 다를수록 증가\n",
    "$KL(p(y) || q(y)) \n",
    "= -\\int p(y) \\log_2 q(y) \\, dy - \\left( -\\int p(y) \\log_2 p(y) \\, dy \\right) \n",
    "= \\int p(y) \\log_2 \\left(\\dfrac{p(y)}{q(y)}\\right) dy$  : 크로스엔트로피 - 엔트로피 <br>\n",
    "\n",
    ": p가 q에서 얼마나 떨어져 있는가 에 대한 내용 (순서가 바뀌면 안된다) -> 두 확률분포의 거리 (얼마나 다른가) 계산 <br>\n",
    "=> 크로스엔트로피 - 엔트로피 = 상대 엔트로피 (relative entrophy) : 상대적으로 얼마나 다른가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
