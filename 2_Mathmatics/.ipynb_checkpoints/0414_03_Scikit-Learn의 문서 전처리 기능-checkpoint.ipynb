{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BoW (Bag of Words) : 문자 -> 숫자로 변환하는, 가장 기본적인 방법\n",
    "<순서><br>\n",
    "1. 전체 문서 {$D_1, D_2, ... , D_n$} 을 구성하는 고정된 단어장 {$W_1, W_2, ... , W_n$}을 만든다.\n",
    "2. $D_i$라는 개별 문서에 $W_j$ 라는 단어가 있는지 표시 - 있으면: $x_{ij}$ = 1, 없으면 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scikit-Learn의 문서 전처리 기능\n",
    "\n",
    "- DictVectorizer : 단어의 수를 세어놓는 사전에서 BoW 벡터 생성.\n",
    "- CountVectorizer : 문서 집합에서 단어 수를 세서 BoW 벡터 생성.\n",
    "- TfidfVectorizer : 문서 집합에서 단어 수를 세고, TF-IDF방식으로 단어의 가중치를 조정한 BoW 벡터 생성.\n",
    "- HashingVectorizer : Hashing trick을 사용, 빠르게 BoW벡를 생성."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 0.],\n",
       "       [0., 3., 1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'A' : 1, 'B' : 2}, {'B' : 3, 'C' : 1}]  #사전형태의 D가 존재\n",
    "X = v.fit_transform(D) #D의 key에 따른 value를 array형태로 리턴\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform({'C' : 4, 'D' : 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) CountVectorizer\n",
    "<순서><br>\n",
    "1. tokenizing : token으로 쪼갠다. (list형태)\n",
    "2. counting : 개별 token을 센다.\n",
    "3. BoW vectorizing : BoW벡터를 생성.<br>\n",
    "<br>\n",
    "\n",
    "<인수><br>\n",
    "1. stop_words : 문자열{'english}, 리스트 또는 None(디폴트)\n",
    "2. analyzer : 문자열 {'word', 'char', 'char_wb'} 또는 함수\n",
    "3. token_pattern : string - 토큰 정의용 정규표현식\n",
    "4. tokenizer : 토큰 생성 함수\n",
    "5. ngram_range : (min_n, max_n) 튜플, ngram의 범위\n",
    "6. max_df : 정수 또는 [0,1]사이의 실수, 단어장에 포함되기 위한 최대 빈도\n",
    "7. min_df : 정수 또는 [0,1]사이의 실수, 단어장에 포함되기 위한 최소 빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'last': 4,\n",
       " 'one': 5,\n",
       " 'second': 6,\n",
       " 'the': 7,\n",
       " 'third': 8,\n",
       " 'this': 9}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',    \n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_ #문장을 token으로 찢어서 단어장 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)-1. Stop Words : 단어장을 생성할 때 무시할 수 있는 단어들 (영어의 관사, 조사 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'first': 1, 'last': 2, 'one': 3, 'second': 4, 'third': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)-2. 토큰  : analyzer / tokenizer / token_pattern 의 인수를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " '?': 2,\n",
       " 'a': 3,\n",
       " 'c': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'l': 10,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " 'o': 13,\n",
       " 'r': 14,\n",
       " 's': 15,\n",
       " 't': 16,\n",
       " 'u': 17}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'third': 1, 'this': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " '?': 1,\n",
       " 'and': 2,\n",
       " 'document': 3,\n",
       " 'first': 4,\n",
       " 'is': 5,\n",
       " 'last': 6,\n",
       " 'one': 7,\n",
       " 'second': 8,\n",
       " 'the': 9,\n",
       " 'third': 10,\n",
       " 'this': 11}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer : 외부의 token단어장을 사용할 수 있음.\n",
    "import nltk\n",
    "\n",
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)-3. n-그램 : 단어장 생성에 사용한 토큰의 크기를 결정 - 복합어를 인정할 것인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and the': 0,\n",
       " 'first document': 1,\n",
       " 'is the': 2,\n",
       " 'is this': 3,\n",
       " 'last document': 4,\n",
       " 'second document': 5,\n",
       " 'second second': 6,\n",
       " 'the first': 7,\n",
       " 'the last': 8,\n",
       " 'the second': 9,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'this is': 12,\n",
       " 'this the': 13}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'the third': 1, 'third': 2, 'this': 3, 'this the': 4}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#다른 조건과 같이 쓸 수 있음.\n",
    "vect = CountVectorizer(ngram_range=(1, 2), token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)-4. 빈도수 : max_df, min_df인수를 사용, 문서 내 토큰의 빈도수로 사용여부 결정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'document': 0, 'first': 1, 'is': 2, 'this': 3},\n",
       " {'and', 'last', 'one', 'second', 'the', 'third'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_, vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) TfidfVectorizer\n",
    "- TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "- 모든 문서에 공통으로 들어 있을 경우, \"가중치 축소\"(Inverse) <br>\n",
    "$\\text{tf-idf}(d, t) = \\text{tf}(d, t) \\cdot \\text{idf}(t)$ <br>\n",
    "$\\text{tf}(d, t)$ : 단어의 빈도수 (1 document, how many term),   $\\text{idf}(t)$ : inverse document frequency (가중치 축소) <br>\n",
    "$\\text{idf}(d, t) = \\log \\dfrac{n}{1 + \\text{df}(t)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.24151532, 0.        , 0.28709733, 0.        ,\n",
       "        0.        , 0.85737594, 0.20427211, 0.        , 0.28709733],\n",
       "       [0.55666851, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.55666851, 0.        , 0.26525553, 0.55666851, 0.        ],\n",
       "       [0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.45333103, 0.        , 0.        , 0.80465933,\n",
       "        0.        , 0.        , 0.38342448, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 사용 예\n",
    "- 문자열 분석기를 사용하여 웹사이트에 특정 단어가 얼마나 사용되었는지 빈도 측정하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import string\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "f = urlopen(\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\")\n",
    "json = json.loads(f.read())\n",
    "cell = [\"\\n\".join(c[\"source\"]) for c in json[\"cells\"] if c[\"cell_type\"] == \"markdown\"]\n",
    "docs = [w for w in hannanum.nouns(\" \".join(cell)) if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1338ae470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(docs)\n",
    "count = vect.transform(docs).toarray().sum(axis=0) #각 단어별로 나온 횟수를 표시 (앞서 docs는 하나의 단어씩으로 이루어져있음 : sum)\n",
    "idx = np.argsort(-count) #count를 내림차순했을때 각 성분의 idx\n",
    "count = count[idx] #나온 횟수 기준으로 내림차순 정렬 상태로 만듬.\n",
    "\n",
    "feature_name = np.array(vect.get_feature_names())[idx] #idx의 순서대로 햇을때 단어장 속 단어들을 표시\n",
    "plt.bar(range(len(count)), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('컨테이너', 81),\n",
      " ('도커', 41),\n",
      " ('명령', 34),\n",
      " ('이미지', 33),\n",
      " ('사용', 26),\n",
      " ('가동', 14),\n",
      " ('중지', 13),\n",
      " ('mingw64', 13),\n",
      " ('삭제', 12),\n",
      " ('이름', 11),\n",
      " ('아이디', 11),\n",
      " ('다음', 10),\n",
      " ('시작', 9),\n",
      " ('목록', 8),\n",
      " ('옵션', 6),\n",
      " ('a181562ac4d8', 6),\n",
      " ('입력', 6),\n",
      " ('외부', 5),\n",
      " ('출력', 5),\n",
      " ('해당', 5),\n",
      " ('호스트', 5),\n",
      " ('명령어', 5),\n",
      " ('확인', 5),\n",
      " ('경우', 5),\n",
      " ('재시작', 4),\n",
      " ('존재', 4),\n",
      " ('컴퓨터', 4),\n",
      " ('터미널', 4),\n",
      " ('프롬프트', 4),\n",
      " ('포트', 4),\n",
      " ('377ad03459bf', 3),\n",
      " ('가상', 3),\n",
      " ('수행', 3),\n",
      " ('문자열', 3),\n",
      " ('dockeruser', 3),\n",
      " ('항목', 3),\n",
      " ('마찬가지', 3),\n",
      " ('대화형', 3),\n",
      " ('종료', 2),\n",
      " ('상태', 2),\n",
      " ('저장', 2),\n",
      " ('호스트간', 2),\n",
      " ('작업', 2),\n",
      " ('지정', 2),\n",
      " ('생각', 2),\n",
      " ('문헌', 2),\n",
      " ('동작', 2),\n",
      " ('시스템', 2),\n",
      " ('명시해', 2),\n",
      " ('특정', 2),\n",
      " ('관련하', 2),\n",
      " ('이때', 2),\n",
      " ('의미', 2),\n",
      " ('추가', 2),\n",
      " ('조합', 1),\n",
      " ('container', 1),\n",
      " ('폴더', 1),\n",
      " ('a1e4ed2ac65b', 1),\n",
      " ('작동', 1),\n",
      " ('자체', 1),\n",
      " ('자동', 1),\n",
      " ('image', 1),\n",
      " ('정지', 1),\n",
      " ('핵심', 1),\n",
      " ('초간단', 1),\n",
      " ('중복', 1),\n",
      " ('id', 1),\n",
      " ('최소한', 1),\n",
      " ('일부분', 1),\n",
      " ('컨테이', 1),\n",
      " ('daemon', 1),\n",
      " ('컨테이너상', 1),\n",
      " ('한다', 1),\n",
      " ('콜론', 1),\n",
      " ('태그', 1),\n",
      " ('하나', 1),\n",
      " ('툴박스', 1),\n",
      " ('파일', 1),\n",
      " ('포워딩', 1),\n",
      " ('주의해', 1),\n",
      " ('이해', 1),\n",
      " ('누른다', 1),\n",
      " ('이미지는', 1),\n",
      " ('공유', 1),\n",
      " ('브라우저', 1),\n",
      " ('복사', 1),\n",
      " ('문제', 1),\n",
      " ('문자', 1),\n",
      " ('관련', 1),\n",
      " ('명시', 1),\n",
      " ('길벗', 1),\n",
      " ('사용법', 1),\n",
      " ('메시지', 1),\n",
      " ('마지막', 1),\n",
      " ('리눅스', 1),\n",
      " ('나오기', 1),\n",
      " ('도서출판', 1),\n",
      " ('데몬', 1),\n",
      " ('대화적', 1),\n",
      " ('대표적', 1),\n",
      " ('내부', 1),\n",
      " ('머신', 1),\n",
      " ('이재홍', 1),\n",
      " ('사용자', 1),\n",
      " ('생략', 1),\n",
      " ('tag', 1),\n",
      " ('가능', 1),\n",
      " ('의존', 1),\n",
      " ('으로', 1),\n",
      " ('내용', 1),\n",
      " ('원본', 1),\n",
      " ('요약', 1),\n",
      " ('가지', 1),\n",
      " ('사용해', 1),\n",
      " ('오류', 1),\n",
      " ('연결', 1),\n",
      " ('여기', 1),\n",
      " ('개념', 1),\n",
      " ('실행', 1),\n",
      " ('시스템상', 1),\n",
      " ('소개', 1),\n",
      " ('설명', 1),\n",
      " ('생성', 1),\n",
      " ('연습', 1),\n",
      " ('윈도우즈', 1)]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(feature_name, count))) #많이 나온 순서대로 단어장 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
